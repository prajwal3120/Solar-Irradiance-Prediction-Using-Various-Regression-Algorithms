{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd6fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d57537",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda683fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:\\Solar Radiation Prediction\\Dataset\\SolarPrediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af22b068",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIXTime</th>\n",
       "      <th>Data</th>\n",
       "      <th>Time</th>\n",
       "      <th>Radiation</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>WindDirection(Degrees)</th>\n",
       "      <th>Speed</th>\n",
       "      <th>TimeSunRise</th>\n",
       "      <th>TimeSunSet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1475229326</td>\n",
       "      <td>9/29/2016 12:00:00 AM</td>\n",
       "      <td>23:55:26</td>\n",
       "      <td>1.21</td>\n",
       "      <td>48</td>\n",
       "      <td>30.46</td>\n",
       "      <td>59</td>\n",
       "      <td>177.39</td>\n",
       "      <td>5.62</td>\n",
       "      <td>06:13:00</td>\n",
       "      <td>18:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1475229023</td>\n",
       "      <td>9/29/2016 12:00:00 AM</td>\n",
       "      <td>23:50:23</td>\n",
       "      <td>1.21</td>\n",
       "      <td>48</td>\n",
       "      <td>30.46</td>\n",
       "      <td>58</td>\n",
       "      <td>176.78</td>\n",
       "      <td>3.37</td>\n",
       "      <td>06:13:00</td>\n",
       "      <td>18:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1475228726</td>\n",
       "      <td>9/29/2016 12:00:00 AM</td>\n",
       "      <td>23:45:26</td>\n",
       "      <td>1.23</td>\n",
       "      <td>48</td>\n",
       "      <td>30.46</td>\n",
       "      <td>57</td>\n",
       "      <td>158.75</td>\n",
       "      <td>3.37</td>\n",
       "      <td>06:13:00</td>\n",
       "      <td>18:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1475228421</td>\n",
       "      <td>9/29/2016 12:00:00 AM</td>\n",
       "      <td>23:40:21</td>\n",
       "      <td>1.21</td>\n",
       "      <td>48</td>\n",
       "      <td>30.46</td>\n",
       "      <td>60</td>\n",
       "      <td>137.71</td>\n",
       "      <td>3.37</td>\n",
       "      <td>06:13:00</td>\n",
       "      <td>18:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1475228124</td>\n",
       "      <td>9/29/2016 12:00:00 AM</td>\n",
       "      <td>23:35:24</td>\n",
       "      <td>1.17</td>\n",
       "      <td>48</td>\n",
       "      <td>30.46</td>\n",
       "      <td>62</td>\n",
       "      <td>104.95</td>\n",
       "      <td>5.62</td>\n",
       "      <td>06:13:00</td>\n",
       "      <td>18:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32681</th>\n",
       "      <td>1480587604</td>\n",
       "      <td>12/1/2016 12:00:00 AM</td>\n",
       "      <td>00:20:04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>44</td>\n",
       "      <td>30.43</td>\n",
       "      <td>102</td>\n",
       "      <td>145.42</td>\n",
       "      <td>6.75</td>\n",
       "      <td>06:41:00</td>\n",
       "      <td>17:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32682</th>\n",
       "      <td>1480587301</td>\n",
       "      <td>12/1/2016 12:00:00 AM</td>\n",
       "      <td>00:15:01</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44</td>\n",
       "      <td>30.42</td>\n",
       "      <td>102</td>\n",
       "      <td>117.78</td>\n",
       "      <td>6.75</td>\n",
       "      <td>06:41:00</td>\n",
       "      <td>17:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32683</th>\n",
       "      <td>1480587001</td>\n",
       "      <td>12/1/2016 12:00:00 AM</td>\n",
       "      <td>00:10:01</td>\n",
       "      <td>1.20</td>\n",
       "      <td>44</td>\n",
       "      <td>30.42</td>\n",
       "      <td>102</td>\n",
       "      <td>145.19</td>\n",
       "      <td>9.00</td>\n",
       "      <td>06:41:00</td>\n",
       "      <td>17:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32684</th>\n",
       "      <td>1480586702</td>\n",
       "      <td>12/1/2016 12:00:00 AM</td>\n",
       "      <td>00:05:02</td>\n",
       "      <td>1.23</td>\n",
       "      <td>44</td>\n",
       "      <td>30.42</td>\n",
       "      <td>101</td>\n",
       "      <td>164.19</td>\n",
       "      <td>7.87</td>\n",
       "      <td>06:41:00</td>\n",
       "      <td>17:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32685</th>\n",
       "      <td>1480586402</td>\n",
       "      <td>12/1/2016 12:00:00 AM</td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>1.20</td>\n",
       "      <td>44</td>\n",
       "      <td>30.43</td>\n",
       "      <td>101</td>\n",
       "      <td>83.59</td>\n",
       "      <td>3.37</td>\n",
       "      <td>06:41:00</td>\n",
       "      <td>17:42:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32686 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UNIXTime                   Data      Time  Radiation  Temperature  \\\n",
       "0      1475229326  9/29/2016 12:00:00 AM  23:55:26       1.21           48   \n",
       "1      1475229023  9/29/2016 12:00:00 AM  23:50:23       1.21           48   \n",
       "2      1475228726  9/29/2016 12:00:00 AM  23:45:26       1.23           48   \n",
       "3      1475228421  9/29/2016 12:00:00 AM  23:40:21       1.21           48   \n",
       "4      1475228124  9/29/2016 12:00:00 AM  23:35:24       1.17           48   \n",
       "...           ...                    ...       ...        ...          ...   \n",
       "32681  1480587604  12/1/2016 12:00:00 AM  00:20:04       1.22           44   \n",
       "32682  1480587301  12/1/2016 12:00:00 AM  00:15:01       1.17           44   \n",
       "32683  1480587001  12/1/2016 12:00:00 AM  00:10:01       1.20           44   \n",
       "32684  1480586702  12/1/2016 12:00:00 AM  00:05:02       1.23           44   \n",
       "32685  1480586402  12/1/2016 12:00:00 AM  00:00:02       1.20           44   \n",
       "\n",
       "       Pressure  Humidity  WindDirection(Degrees)  Speed TimeSunRise  \\\n",
       "0         30.46        59                  177.39   5.62    06:13:00   \n",
       "1         30.46        58                  176.78   3.37    06:13:00   \n",
       "2         30.46        57                  158.75   3.37    06:13:00   \n",
       "3         30.46        60                  137.71   3.37    06:13:00   \n",
       "4         30.46        62                  104.95   5.62    06:13:00   \n",
       "...         ...       ...                     ...    ...         ...   \n",
       "32681     30.43       102                  145.42   6.75    06:41:00   \n",
       "32682     30.42       102                  117.78   6.75    06:41:00   \n",
       "32683     30.42       102                  145.19   9.00    06:41:00   \n",
       "32684     30.42       101                  164.19   7.87    06:41:00   \n",
       "32685     30.43       101                   83.59   3.37    06:41:00   \n",
       "\n",
       "      TimeSunSet  \n",
       "0       18:13:00  \n",
       "1       18:13:00  \n",
       "2       18:13:00  \n",
       "3       18:13:00  \n",
       "4       18:13:00  \n",
       "...          ...  \n",
       "32681   17:42:00  \n",
       "32682   17:42:00  \n",
       "32683   17:42:00  \n",
       "32684   17:42:00  \n",
       "32685   17:42:00  \n",
       "\n",
       "[32686 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dce0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Month'] = data['Data'].apply(lambda x: re.search(r'^\\d+', x).group(0)).astype(int)\n",
    "data['Day'] = data['Data'].apply(lambda x: re.search(r'(?<=\\/)\\d+(?=\\/)', x).group(0)).astype(int)\n",
    "data['Year'] = data['Data'].apply(lambda x: re.search(r'(?<=\\/)\\d+(?=\\s)', x).group(0)).astype(int)\n",
    "\n",
    "data = data.drop('Data', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b28c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Hour'] = data['Time'].apply(lambda x: re.search(r'^\\d+', x).group(0)).astype(int)\n",
    "data['Minute'] = data['Time'].apply(lambda x: re.search(r'(?<=:)\\d+(?=:)', x).group(0)).astype(int)\n",
    "data['Second'] = data['Time'].apply(lambda x: re.search(r'\\d+$', x).group(0)).astype(int)\n",
    "\n",
    "data = data.drop('Time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4bbec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['SunriseHour'] = data['TimeSunRise'].apply(lambda x: re.search(r'^\\d+', x).group(0)).astype(int)\n",
    "data['SunriseMinute'] = data['TimeSunRise'].apply(lambda x: re.search(r'(?<=:)\\d+(?=:)', x).group(0)).astype(int)\n",
    "\n",
    "data['SunsetHour'] = data['TimeSunSet'].apply(lambda x: re.search(r'^\\d+', x).group(0)).astype(int)\n",
    "data['SunsetMinute'] = data['TimeSunSet'].apply(lambda x: re.search(r'(?<=:)\\d+(?=:)', x).group(0)).astype(int)\n",
    "\n",
    "data = data.drop(['TimeSunRise', 'TimeSunSet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6bf206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Year', 'SunriseHour'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01263fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Radiation'].copy()\n",
    "X = data.drop('Radiation', axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b360e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "174699ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=100)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8, random_state=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd1cf5",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a144d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "y_true = np.array(y_test, dtype=float)\n",
    "r2_linear = r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92a9ea",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b6fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "regressor = DecisionTreeRegressor(random_state = 0) \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_true = np.array(y_test, dtype=float)\n",
    "r2_tree = r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef80c328",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9f817e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svregressor = SVR(kernel = 'rbf')\n",
    "svregressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_true = np.array(y_test, dtype=float)\n",
    "r2_svr = r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427f3f1",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "403c5164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "916/916 [==============================] - 1s 866us/step - loss: 131195.6250\n",
      "Epoch 2/50\n",
      "916/916 [==============================] - 1s 831us/step - loss: 116537.6328\n",
      "Epoch 3/50\n",
      "916/916 [==============================] - 1s 679us/step - loss: 105566.9922\n",
      "Epoch 4/50\n",
      "916/916 [==============================] - 1s 757us/step - loss: 97207.9609\n",
      "Epoch 5/50\n",
      "916/916 [==============================] - 1s 666us/step - loss: 91014.2500\n",
      "Epoch 6/50\n",
      "916/916 [==============================] - 1s 662us/step - loss: 86659.7266\n",
      "Epoch 7/50\n",
      "916/916 [==============================] - 1s 766us/step - loss: 83775.9219\n",
      "Epoch 8/50\n",
      "916/916 [==============================] - 1s 671us/step - loss: 82021.9766\n",
      "Epoch 9/50\n",
      "916/916 [==============================] - 1s 762us/step - loss: 81063.8125\n",
      "Epoch 10/50\n",
      "916/916 [==============================] - 1s 679us/step - loss: 80582.4375\n",
      "Epoch 11/50\n",
      "916/916 [==============================] - 1s 764us/step - loss: 80359.8828\n",
      "Epoch 12/50\n",
      "916/916 [==============================] - 1s 735us/step - loss: 80250.0234\n",
      "Epoch 13/50\n",
      "916/916 [==============================] - 1s 686us/step - loss: 80205.8750\n",
      "Epoch 14/50\n",
      "916/916 [==============================] - 1s 757us/step - loss: 80179.0234\n",
      "Epoch 15/50\n",
      "916/916 [==============================] - 1s 653us/step - loss: 80134.4844\n",
      "Epoch 16/50\n",
      "916/916 [==============================] - 1s 618us/step - loss: 80106.5312\n",
      "Epoch 17/50\n",
      "916/916 [==============================] - 1s 619us/step - loss: 80105.7500\n",
      "Epoch 18/50\n",
      "916/916 [==============================] - 1s 628us/step - loss: 80080.7500\n",
      "Epoch 19/50\n",
      "916/916 [==============================] - 1s 773us/step - loss: 80054.4219\n",
      "Epoch 20/50\n",
      "916/916 [==============================] - 1s 728us/step - loss: 80060.5469\n",
      "Epoch 21/50\n",
      "916/916 [==============================] - 1s 741us/step - loss: 80041.2109\n",
      "Epoch 22/50\n",
      "916/916 [==============================] - 1s 779us/step - loss: 80034.8984\n",
      "Epoch 23/50\n",
      "916/916 [==============================] - 1s 734us/step - loss: 80022.8516\n",
      "Epoch 24/50\n",
      "916/916 [==============================] - 1s 671us/step - loss: 80018.3359\n",
      "Epoch 25/50\n",
      "916/916 [==============================] - 1s 772us/step - loss: 80000.8438\n",
      "Epoch 26/50\n",
      "916/916 [==============================] - 1s 740us/step - loss: 80005.0156\n",
      "Epoch 27/50\n",
      "916/916 [==============================] - 1s 693us/step - loss: 79992.4609\n",
      "Epoch 28/50\n",
      "916/916 [==============================] - 1s 782us/step - loss: 79983.4219\n",
      "Epoch 29/50\n",
      "916/916 [==============================] - 1s 738us/step - loss: 79977.6250\n",
      "Epoch 30/50\n",
      "916/916 [==============================] - 1s 689us/step - loss: 79959.4922\n",
      "Epoch 31/50\n",
      "916/916 [==============================] - 1s 773us/step - loss: 79961.3594\n",
      "Epoch 32/50\n",
      "916/916 [==============================] - 1s 747us/step - loss: 79951.8984\n",
      "Epoch 33/50\n",
      "916/916 [==============================] - 1s 689us/step - loss: 79902.1953\n",
      "Epoch 34/50\n",
      "916/916 [==============================] - 1s 759us/step - loss: 79447.1328\n",
      "Epoch 35/50\n",
      "916/916 [==============================] - 1s 742us/step - loss: 63849.2578\n",
      "Epoch 36/50\n",
      "916/916 [==============================] - 1s 668us/step - loss: 48151.1367\n",
      "Epoch 37/50\n",
      "916/916 [==============================] - 1s 703us/step - loss: 42834.1055\n",
      "Epoch 38/50\n",
      "916/916 [==============================] - 1s 721us/step - loss: 38637.3906\n",
      "Epoch 39/50\n",
      "916/916 [==============================] - 1s 677us/step - loss: 35091.1484\n",
      "Epoch 40/50\n",
      "916/916 [==============================] - 1s 782us/step - loss: 32086.0840\n",
      "Epoch 41/50\n",
      "916/916 [==============================] - 1s 730us/step - loss: 29450.1465\n",
      "Epoch 42/50\n",
      "916/916 [==============================] - 1s 678us/step - loss: 27117.9180\n",
      "Epoch 43/50\n",
      "916/916 [==============================] - 1s 789us/step - loss: 25001.1934\n",
      "Epoch 44/50\n",
      "916/916 [==============================] - 1s 748us/step - loss: 23121.5410\n",
      "Epoch 45/50\n",
      "916/916 [==============================] - 1s 680us/step - loss: 21456.0957\n",
      "Epoch 46/50\n",
      "916/916 [==============================] - 1s 771us/step - loss: 19958.2051\n",
      "Epoch 47/50\n",
      "916/916 [==============================] - 1s 763us/step - loss: 18599.7812 0s - loss: 18526.\n",
      "Epoch 48/50\n",
      "916/916 [==============================] - 1s 682us/step - loss: 17346.7383\n",
      "Epoch 49/50\n",
      "916/916 [==============================] - 1s 769us/step - loss: 16121.1221\n",
      "Epoch 50/50\n",
      "916/916 [==============================] - 1s 762us/step - loss: 15217.9951\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Dense(units=50, input_dim=14, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model.add(Dense(units=50, kernel_initializer='normal', activation='tanh'))\n",
    " \n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    " \n",
    "model.fit(X_train, y_train, batch_size = 20, epochs = 50, verbose=1)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_true = np.array(y_test, dtype=float)\n",
    "\n",
    "r2_ann = r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e34591",
   "metadata": {},
   "source": [
    "# Optuna (For Tuning Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c2107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "471fae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_rmse(params):\n",
    "    mod = xgb.train(params, dtrain, num_boost_round=100, evals=[(dval, 'eval')], early_stopping_rounds=10, verbose_eval=0)\n",
    "    results = mod.eval(dval)\n",
    "    rmse = float(re.search(r'[\\d.]+$', results).group(0))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e5e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.00001, 10.0)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 8)\n",
    "    l1_reg = trial.suggest_loguniform('l1_reg', 0.00001, 10.0)\n",
    "    l2_reg = trial.suggest_loguniform('l2_reg', 0.00001, 10.0)\n",
    "    \n",
    "    params = {'learning_rate': learning_rate, 'max_depth': max_depth, 'alpha': l1_reg, 'lambda': l2_reg}\n",
    "    \n",
    "    return get_model_rmse(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "713d9c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-02 18:09:25,283]\u001b[0m A new study created in memory with name: no-name-55738730-1b1c-4de7-8610-6aa6468f29d3\u001b[0m\n",
      "C:\\Users\\Prajwal\\anaconda3\\lib\\site-packages\\optuna\\progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c2f43ad5f94f8eb0f36ba87ef32b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-02 18:09:27,212]\u001b[0m Trial 0 finished with value: 93.191338 and parameters: {'learning_rate': 0.026990417326274923, 'max_depth': 8, 'l1_reg': 0.22427037289040827, 'l2_reg': 0.0018697823609213982}. Best is trial 0 with value: 93.191338.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:29,028]\u001b[0m Trial 1 finished with value: 184.741409 and parameters: {'learning_rate': 0.008706547624370013, 'max_depth': 8, 'l1_reg': 0.06793681851197442, 'l2_reg': 0.0004648746185824279}. Best is trial 0 with value: 93.191338.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:29,217]\u001b[0m Trial 2 finished with value: 19240212480.0 and parameters: {'learning_rate': 6.11494594973384, 'max_depth': 8, 'l1_reg': 0.00013967609851377112, 'l2_reg': 4.142281105573771}. Best is trial 0 with value: 93.191338.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:29,637]\u001b[0m Trial 3 finished with value: 101.213387 and parameters: {'learning_rate': 0.777234142826214, 'max_depth': 4, 'l1_reg': 0.04365932775317912, 'l2_reg': 0.037708914636973206}. Best is trial 0 with value: 93.191338.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:31,132]\u001b[0m Trial 4 finished with value: 87.481483 and parameters: {'learning_rate': 0.07744092317422278, 'max_depth': 7, 'l1_reg': 0.1325594211852168, 'l2_reg': 0.13150904051171208}. Best is trial 4 with value: 87.481483.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:32,579]\u001b[0m Trial 5 finished with value: 380.894562 and parameters: {'learning_rate': 1.7097432746212602e-05, 'max_depth': 7, 'l1_reg': 7.067283415281067, 'l2_reg': 0.0065745456225947095}. Best is trial 4 with value: 87.481483.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:33,369]\u001b[0m Trial 6 finished with value: 101.008186 and parameters: {'learning_rate': 0.11343626113160021, 'max_depth': 4, 'l1_reg': 2.245707472811267, 'l2_reg': 1.051651269316006}. Best is trial 4 with value: 87.481483.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:34,601]\u001b[0m Trial 7 finished with value: 372.294617 and parameters: {'learning_rate': 0.00027435935176256814, 'max_depth': 6, 'l1_reg': 4.6868545215139325e-05, 'l2_reg': 1.1711724030845083}. Best is trial 4 with value: 87.481483.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:34,988]\u001b[0m Trial 8 finished with value: 147.799683 and parameters: {'learning_rate': 1.875745840660046, 'max_depth': 6, 'l1_reg': 0.00016194739960541436, 'l2_reg': 0.015161058892133265}. Best is trial 4 with value: 87.481483.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:35,759]\u001b[0m Trial 9 finished with value: 213.472198 and parameters: {'learning_rate': 0.007836479202501221, 'max_depth': 4, 'l1_reg': 0.020457820606080375, 'l2_reg': 0.003909033741583204}. Best is trial 4 with value: 87.481483.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:37,405]\u001b[0m Trial 10 finished with value: 346.989685 and parameters: {'learning_rate': 0.0010566926216860836, 'max_depth': 7, 'l1_reg': 0.0019328428184636893, 'l2_reg': 1.7895121363831237e-05}. Best is trial 4 with value: 87.481483.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:38,928]\u001b[0m Trial 11 finished with value: 87.824059 and parameters: {'learning_rate': 0.0835485217923309, 'max_depth': 7, 'l1_reg': 0.3300491057810451, 'l2_reg': 0.08539543137773696}. Best is trial 4 with value: 87.481483.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:40,007]\u001b[0m Trial 12 finished with value: 87.362167 and parameters: {'learning_rate': 0.19057401458428835, 'max_depth': 7, 'l1_reg': 0.5937828295324086, 'l2_reg': 0.13597403412888268}. Best is trial 12 with value: 87.362167.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:40,854]\u001b[0m Trial 13 finished with value: 92.462021 and parameters: {'learning_rate': 0.42645786773077304, 'max_depth': 5, 'l1_reg': 0.0029447758795236216, 'l2_reg': 0.17531209590164923}. Best is trial 12 with value: 87.362167.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:42,383]\u001b[0m Trial 14 finished with value: 330.625519 and parameters: {'learning_rate': 0.0016053885438719584, 'max_depth': 7, 'l1_reg': 0.8546450724842213, 'l2_reg': 0.31086774158604347}. Best is trial 12 with value: 87.362167.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:43,641]\u001b[0m Trial 15 finished with value: 86.335091 and parameters: {'learning_rate': 0.18420801119608665, 'max_depth': 6, 'l1_reg': 0.005959885148752637, 'l2_reg': 0.00019126120878470705}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:43,783]\u001b[0m Trial 16 finished with value: 9761493876736.0 and parameters: {'learning_rate': 9.887062312590073, 'max_depth': 5, 'l1_reg': 0.005806888428749916, 'l2_reg': 3.569311400370212e-05}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:44,574]\u001b[0m Trial 17 finished with value: 92.152313 and parameters: {'learning_rate': 0.37366956781884414, 'max_depth': 5, 'l1_reg': 0.0008007516773395252, 'l2_reg': 0.00019225477629910333}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:45,866]\u001b[0m Trial 18 finished with value: 100.319817 and parameters: {'learning_rate': 0.03031581961317922, 'max_depth': 6, 'l1_reg': 0.012727460137621395, 'l2_reg': 0.00014284048284267422}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:46,033]\u001b[0m Trial 19 finished with value: 8114.686523 and parameters: {'learning_rate': 2.3554947361646787, 'max_depth': 6, 'l1_reg': 1.3926867206368243e-05, 'l2_reg': 7.981267706449421}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:47,266]\u001b[0m Trial 20 finished with value: 87.172501 and parameters: {'learning_rate': 0.27394609155348754, 'max_depth': 6, 'l1_reg': 0.0006934773426209422, 'l2_reg': 0.0016641760502876452}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:48,330]\u001b[0m Trial 21 finished with value: 88.276726 and parameters: {'learning_rate': 0.25492316037357793, 'max_depth': 6, 'l1_reg': 0.0007182780294200402, 'l2_reg': 0.0008871060108905799}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:48,549]\u001b[0m Trial 22 finished with value: 125.783592 and parameters: {'learning_rate': 1.4797500042012066, 'max_depth': 5, 'l1_reg': 0.0007737934743653292, 'l2_reg': 0.0001484221886527648}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:50,088]\u001b[0m Trial 23 finished with value: 96.110909 and parameters: {'learning_rate': 0.028711730010806437, 'max_depth': 7, 'l1_reg': 0.00020418434050318107, 'l2_reg': 0.027108308571027133}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:51,314]\u001b[0m Trial 24 finished with value: 88.556801 and parameters: {'learning_rate': 0.16763667130375506, 'max_depth': 6, 'l1_reg': 0.006477817622683316, 'l2_reg': 0.0018306916093520796}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:52,342]\u001b[0m Trial 25 finished with value: 287.223724 and parameters: {'learning_rate': 0.003398056989989443, 'max_depth': 5, 'l1_reg': 0.022684554683728413, 'l2_reg': 4.649594606304889e-05}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:52,977]\u001b[0m Trial 26 finished with value: 92.709953 and parameters: {'learning_rate': 0.623948546103004, 'max_depth': 7, 'l1_reg': 0.0025446082919821757, 'l2_reg': 0.000753095001448411}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:54,275]\u001b[0m Trial 27 finished with value: 92.33004 and parameters: {'learning_rate': 0.052083743731412994, 'max_depth': 6, 'l1_reg': 5.7055660223913645, 'l2_reg': 0.004826089029125666}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:54,445]\u001b[0m Trial 28 finished with value: 7285483.0 and parameters: {'learning_rate': 3.462570437622935, 'max_depth': 6, 'l1_reg': 1.1947992965032512, 'l2_reg': 0.00025888621821023867}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:54,764]\u001b[0m Trial 29 finished with value: 101.532455 and parameters: {'learning_rate': 0.8200692155530978, 'max_depth': 8, 'l1_reg': 0.000412222133800513, 'l2_reg': 0.0018383418734956183}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:56,562]\u001b[0m Trial 30 finished with value: 121.618492 and parameters: {'learning_rate': 0.015961244493380686, 'max_depth': 8, 'l1_reg': 0.14570019371270962, 'l2_reg': 6.174792277047646e-05}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:58,003]\u001b[0m Trial 31 finished with value: 87.730148 and parameters: {'learning_rate': 0.0858044984281754, 'max_depth': 7, 'l1_reg': 0.23066462347079839, 'l2_reg': 0.4043130783523745}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:09:59,285]\u001b[0m Trial 32 finished with value: 86.771072 and parameters: {'learning_rate': 0.195234544546505, 'max_depth': 7, 'l1_reg': 0.07198427819311294, 'l2_reg': 0.05963787852952936}. Best is trial 15 with value: 86.335091.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:00,987]\u001b[0m Trial 33 finished with value: 84.83831 and parameters: {'learning_rate': 0.20506743957502854, 'max_depth': 8, 'l1_reg': 0.046853026039838215, 'l2_reg': 0.04224748286236167}. Best is trial 33 with value: 84.83831.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:02,839]\u001b[0m Trial 34 finished with value: 154.198868 and parameters: {'learning_rate': 0.011419553686026205, 'max_depth': 8, 'l1_reg': 0.059389556517906936, 'l2_reg': 0.04409874993071244}. Best is trial 33 with value: 84.83831.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:03,098]\u001b[0m Trial 35 finished with value: 116.712166 and parameters: {'learning_rate': 1.1260680945247816, 'max_depth': 8, 'l1_reg': 0.03962901281904347, 'l2_reg': 0.015356533362483825}. Best is trial 33 with value: 84.83831.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:04,944]\u001b[0m Trial 36 finished with value: 86.62326 and parameters: {'learning_rate': 0.04314807909701599, 'max_depth': 8, 'l1_reg': 0.10997258958120462, 'l2_reg': 0.0005574772917184878}. Best is trial 33 with value: 84.83831.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:06,751]\u001b[0m Trial 37 finished with value: 86.987679 and parameters: {'learning_rate': 0.04370168344988564, 'max_depth': 8, 'l1_reg': 0.10901171404024419, 'l2_reg': 0.01027570407256074}. Best is trial 33 with value: 84.83831.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:08,562]\u001b[0m Trial 38 finished with value: 381.077118 and parameters: {'learning_rate': 1.1645605033611786e-05, 'max_depth': 8, 'l1_reg': 0.03149609899530925, 'l2_reg': 0.04269731059803258}. Best is trial 33 with value: 84.83831.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:10,394]\u001b[0m Trial 39 finished with value: 378.230682 and parameters: {'learning_rate': 9.37100235655591e-05, 'max_depth': 8, 'l1_reg': 0.010184464822238786, 'l2_reg': 0.0004554963831913413}. Best is trial 33 with value: 84.83831.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:12,225]\u001b[0m Trial 40 finished with value: 245.522308 and parameters: {'learning_rate': 0.005027823323681093, 'max_depth': 8, 'l1_reg': 0.08656902344511502, 'l2_reg': 0.004326527776812353}. Best is trial 33 with value: 84.83831.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:14,021]\u001b[0m Trial 41 finished with value: 85.541847 and parameters: {'learning_rate': 0.04586468264170558, 'max_depth': 8, 'l1_reg': 0.4378167569261722, 'l2_reg': 0.010149006943335174}. Best is trial 33 with value: 84.83831.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:15,721]\u001b[0m Trial 42 finished with value: 84.40596 and parameters: {'learning_rate': 0.09804328599487999, 'max_depth': 8, 'l1_reg': 0.39692076343080296, 'l2_reg': 0.024489051673082064}. Best is trial 42 with value: 84.40596.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:17,474]\u001b[0m Trial 43 finished with value: 106.803307 and parameters: {'learning_rate': 0.019555854855583213, 'max_depth': 8, 'l1_reg': 0.5058048318310914, 'l2_reg': 0.02096010897927433}. Best is trial 42 with value: 84.40596.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:19,190]\u001b[0m Trial 44 finished with value: 84.370888 and parameters: {'learning_rate': 0.10651804078157208, 'max_depth': 8, 'l1_reg': 1.4984604455349793, 'l2_reg': 1.2013905521017204e-05}. Best is trial 44 with value: 84.370888.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:20,922]\u001b[0m Trial 45 finished with value: 84.381233 and parameters: {'learning_rate': 0.12770582257228058, 'max_depth': 8, 'l1_reg': 2.5362350804193747, 'l2_reg': 1.5166940652163292e-05}. Best is trial 44 with value: 84.370888.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:22,702]\u001b[0m Trial 46 finished with value: 85.276085 and parameters: {'learning_rate': 0.07026504644796502, 'max_depth': 8, 'l1_reg': 3.056364387809684, 'l2_reg': 1.0238953841228205e-05}. Best is trial 44 with value: 84.370888.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:24,425]\u001b[0m Trial 47 finished with value: 84.242195 and parameters: {'learning_rate': 0.10770037851667165, 'max_depth': 8, 'l1_reg': 2.4251810313679005, 'l2_reg': 1.2942762641082106e-05}. Best is trial 47 with value: 84.242195.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:26,127]\u001b[0m Trial 48 finished with value: 84.36927 and parameters: {'learning_rate': 0.105055489937162, 'max_depth': 8, 'l1_reg': 9.653804086002683, 'l2_reg': 1.3403345565764557e-05}. Best is trial 47 with value: 84.242195.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:27,819]\u001b[0m Trial 49 finished with value: 83.611588 and parameters: {'learning_rate': 0.11124940270545293, 'max_depth': 8, 'l1_reg': 9.314397753503524, 'l2_reg': 2.2584971571392447e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:28,560]\u001b[0m Trial 50 finished with value: 91.404984 and parameters: {'learning_rate': 0.44971988778237154, 'max_depth': 7, 'l1_reg': 8.696880123810919, 'l2_reg': 2.1635293168214218e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:30,264]\u001b[0m Trial 51 finished with value: 85.181419 and parameters: {'learning_rate': 0.11544179797634836, 'max_depth': 8, 'l1_reg': 2.65055955740585, 'l2_reg': 1.0213119252912326e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:31,271]\u001b[0m Trial 52 finished with value: 87.388039 and parameters: {'learning_rate': 0.10908963727669899, 'max_depth': 8, 'l1_reg': 1.475282177596525, 'l2_reg': 8.321224938989433e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:33,028]\u001b[0m Trial 53 finished with value: 105.136604 and parameters: {'learning_rate': 0.020200239573561925, 'max_depth': 8, 'l1_reg': 4.300215284577275, 'l2_reg': 1.7553407062659814e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:34,719]\u001b[0m Trial 54 finished with value: 84.555252 and parameters: {'learning_rate': 0.1260588349314568, 'max_depth': 8, 'l1_reg': 1.665852812949815, 'l2_reg': 2.33535430923126e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:35,245]\u001b[0m Trial 55 finished with value: 90.122971 and parameters: {'learning_rate': 0.40055021710444155, 'max_depth': 8, 'l1_reg': 4.3162025300944835, 'l2_reg': 3.0073218306086748e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:36,726]\u001b[0m Trial 56 finished with value: 159.03511 and parameters: {'learning_rate': 0.011350215366253878, 'max_depth': 7, 'l1_reg': 6.277205343456638, 'l2_reg': 1.5034626588249956e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:38,467]\u001b[0m Trial 57 finished with value: 85.566826 and parameters: {'learning_rate': 0.06861907864550829, 'max_depth': 8, 'l1_reg': 0.8301287729573964, 'l2_reg': 8.981907273076613e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:38,992]\u001b[0m Trial 58 finished with value: 95.199234 and parameters: {'learning_rate': 0.5657218263869398, 'max_depth': 8, 'l1_reg': 2.1979479798145207, 'l2_reg': 4.089402950053109e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:39,751]\u001b[0m Trial 59 finished with value: 88.333885 and parameters: {'learning_rate': 0.29718255824594075, 'max_depth': 7, 'l1_reg': 8.891518917735649, 'l2_reg': 6.585452880386525e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:41,556]\u001b[0m Trial 60 finished with value: 89.061096 and parameters: {'learning_rate': 0.03454842909461732, 'max_depth': 8, 'l1_reg': 1.0080878632785286, 'l2_reg': 1.3273151863994729e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:43,245]\u001b[0m Trial 61 finished with value: 84.117813 and parameters: {'learning_rate': 0.13855017776884299, 'max_depth': 8, 'l1_reg': 1.9000622372288278, 'l2_reg': 2.4112091494376405e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:44,815]\u001b[0m Trial 62 finished with value: 85.768173 and parameters: {'learning_rate': 0.1636770368817349, 'max_depth': 8, 'l1_reg': 4.000458889830627, 'l2_reg': 3.214591584981791e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:45,097]\u001b[0m Trial 63 finished with value: 107.306389 and parameters: {'learning_rate': 0.9199196174664439, 'max_depth': 8, 'l1_reg': 9.907123115714937, 'l2_reg': 2.218991905076403e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:46,837]\u001b[0m Trial 64 finished with value: 84.164627 and parameters: {'learning_rate': 0.08403221389460337, 'max_depth': 8, 'l1_reg': 2.653402170326007, 'l2_reg': 4.9380649726735966e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:48,306]\u001b[0m Trial 65 finished with value: 88.361282 and parameters: {'learning_rate': 0.06827430923339256, 'max_depth': 7, 'l1_reg': 2.677223034984815, 'l2_reg': 0.00010941148442429683}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:49,132]\u001b[0m Trial 66 finished with value: 228.556503 and parameters: {'learning_rate': 0.006771433339058513, 'max_depth': 4, 'l1_reg': 1.7997855558795455, 'l2_reg': 4.636893483892637e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:50,914]\u001b[0m Trial 67 finished with value: 95.800232 and parameters: {'learning_rate': 0.024968110394865926, 'max_depth': 8, 'l1_reg': 5.62142324360058, 'l2_reg': 1.0108351915915802e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:52,411]\u001b[0m Trial 68 finished with value: 85.286949 and parameters: {'learning_rate': 0.25047109373599263, 'max_depth': 8, 'l1_reg': 0.8391550894474576, 'l2_reg': 3.1019246713293325e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:53,481]\u001b[0m Trial 69 finished with value: 88.04277 and parameters: {'learning_rate': 0.31224348900227555, 'max_depth': 7, 'l1_reg': 3.497413868280902, 'l2_reg': 5.493945425265752e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:53,731]\u001b[0m Trial 70 finished with value: 32372.441406 and parameters: {'learning_rate': 2.503501350633995, 'max_depth': 8, 'l1_reg': 0.22120875814578148, 'l2_reg': 0.00025318958166755724}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:55,454]\u001b[0m Trial 71 finished with value: 83.866188 and parameters: {'learning_rate': 0.12583207654325435, 'max_depth': 8, 'l1_reg': 0.6388529028038415, 'l2_reg': 1.5704269355025637e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:56,512]\u001b[0m Trial 72 finished with value: 86.753014 and parameters: {'learning_rate': 0.15902106838943508, 'max_depth': 8, 'l1_reg': 1.334429734960414, 'l2_reg': 1.7780311500133158e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:10:58,274]\u001b[0m Trial 73 finished with value: 83.96563 and parameters: {'learning_rate': 0.14042124459229977, 'max_depth': 8, 'l1_reg': 0.6559205453643765, 'l2_reg': 1.4210167671050141e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:00,131]\u001b[0m Trial 74 finished with value: 86.289391 and parameters: {'learning_rate': 0.05353565069067465, 'max_depth': 8, 'l1_reg': 0.628010102075716, 'l2_reg': 2.7776098001085823e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:00,574]\u001b[0m Trial 75 finished with value: 94.775063 and parameters: {'learning_rate': 0.6172276939960815, 'max_depth': 8, 'l1_reg': 0.2790650688475116, 'l2_reg': 4.2515748734636515e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:02,448]\u001b[0m Trial 76 finished with value: 90.989304 and parameters: {'learning_rate': 0.029422294460231403, 'max_depth': 8, 'l1_reg': 5.835116197514007, 'l2_reg': 1.3831316148648088e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:04,291]\u001b[0m Trial 77 finished with value: 84.632271 and parameters: {'learning_rate': 0.07286262607535687, 'max_depth': 8, 'l1_reg': 0.6797304659871222, 'l2_reg': 0.00013086209433608743}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:05,455]\u001b[0m Trial 78 finished with value: 86.62056 and parameters: {'learning_rate': 0.24474839993663117, 'max_depth': 8, 'l1_reg': 1.956112535635254, 'l2_reg': 7.281473052357076e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:05,700]\u001b[0m Trial 79 finished with value: 128.774811 and parameters: {'learning_rate': 1.3910613433401697, 'max_depth': 7, 'l1_reg': 1.1155823158717768, 'l2_reg': 2.2476576754672433e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:07,587]\u001b[0m Trial 80 finished with value: 128.857895 and parameters: {'learning_rate': 0.014698546481555516, 'max_depth': 8, 'l1_reg': 0.1711657071554438, 'l2_reg': 4.2772494255380896e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:09,195]\u001b[0m Trial 81 finished with value: 84.984253 and parameters: {'learning_rate': 0.15153435455868872, 'max_depth': 8, 'l1_reg': 3.049135244816273, 'l2_reg': 1.5744638251593192e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:11,053]\u001b[0m Trial 82 finished with value: 84.924614 and parameters: {'learning_rate': 0.10384734334977427, 'max_depth': 8, 'l1_reg': 5.19627858937386, 'l2_reg': 1.3395137675703182e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:12,882]\u001b[0m Trial 83 finished with value: 85.790657 and parameters: {'learning_rate': 0.05350664664347643, 'max_depth': 8, 'l1_reg': 2.079136627755313, 'l2_reg': 2.5291830664016134e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:13,467]\u001b[0m Trial 84 finished with value: 88.982399 and parameters: {'learning_rate': 0.378947092853215, 'max_depth': 8, 'l1_reg': 7.097245708477471, 'l2_reg': 1.1257194015277764e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:14,678]\u001b[0m Trial 85 finished with value: 85.285049 and parameters: {'learning_rate': 0.13891962039899847, 'max_depth': 8, 'l1_reg': 1.3499679181140956, 'l2_reg': 1.6901041107322207e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:15,885]\u001b[0m Trial 86 finished with value: 84.02272 and parameters: {'learning_rate': 0.20857593561499366, 'max_depth': 8, 'l1_reg': 6.486453196233789e-05, 'l2_reg': 3.630414511472086e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:17,210]\u001b[0m Trial 87 finished with value: 84.495056 and parameters: {'learning_rate': 0.1927582877965823, 'max_depth': 8, 'l1_reg': 1.701350530476355e-05, 'l2_reg': 5.758886464178628e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:17,649]\u001b[0m Trial 88 finished with value: 90.673431 and parameters: {'learning_rate': 0.5015533903665575, 'max_depth': 8, 'l1_reg': 0.32330996178848076, 'l2_reg': 3.4872615574054425e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:19,389]\u001b[0m Trial 89 finished with value: 85.178139 and parameters: {'learning_rate': 0.08612567961243285, 'max_depth': 8, 'l1_reg': 2.3329718284263367e-05, 'l2_reg': 2.1462871974583066e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:21,204]\u001b[0m Trial 90 finished with value: 88.366364 and parameters: {'learning_rate': 0.03500693443774285, 'max_depth': 8, 'l1_reg': 0.0003553333615451636, 'l2_reg': 0.0002106502798981956}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:22,422]\u001b[0m Trial 91 finished with value: 85.235138 and parameters: {'learning_rate': 0.22372217628845448, 'max_depth': 8, 'l1_reg': 3.662921918865401, 'l2_reg': 1.005699877646554e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:24,170]\u001b[0m Trial 92 finished with value: 84.25679 and parameters: {'learning_rate': 0.10555127650407992, 'max_depth': 8, 'l1_reg': 4.809966463155195e-05, 'l2_reg': 1.844384384402442e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:25,987]\u001b[0m Trial 93 finished with value: 86.048843 and parameters: {'learning_rate': 0.05818613949250766, 'max_depth': 8, 'l1_reg': 6.285723404823793e-05, 'l2_reg': 3.2165248139192565e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:27,777]\u001b[0m Trial 94 finished with value: 84.754257 and parameters: {'learning_rate': 0.09563025631908655, 'max_depth': 8, 'l1_reg': 0.003497919803211146, 'l2_reg': 9.35139245432968e-05}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:29,255]\u001b[0m Trial 95 finished with value: 84.180534 and parameters: {'learning_rate': 0.3036310510205742, 'max_depth': 7, 'l1_reg': 7.451249542521948, 'l2_reg': 3.7283157998584016}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:30,077]\u001b[0m Trial 96 finished with value: 94.591873 and parameters: {'learning_rate': 0.76654283836989, 'max_depth': 7, 'l1_reg': 7.907769808896282, 'l2_reg': 2.238963832616319}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:30,768]\u001b[0m Trial 97 finished with value: 89.239906 and parameters: {'learning_rate': 0.3298899020559367, 'max_depth': 7, 'l1_reg': 0.0013718880310187756, 'l2_reg': 0.29634722955335463}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:32,601]\u001b[0m Trial 98 finished with value: 86.627731 and parameters: {'learning_rate': 0.04249138543639513, 'max_depth': 8, 'l1_reg': 7.529439824985341e-05, 'l2_reg': 1.0351194052098776}. Best is trial 49 with value: 83.611588.\u001b[0m\n",
      "\u001b[32m[I 2022-01-02 18:11:33,729]\u001b[0m Trial 99 finished with value: 87.032806 and parameters: {'learning_rate': 0.2041037533257354, 'max_depth': 7, 'l1_reg': 4.199287892240662e-05, 'l2_reg': 0.0012800782329974389}. Best is trial 49 with value: 83.611588.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd8f87f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.11124940270545293,\n",
       " 'max_depth': 8,\n",
       " 'l1_reg': 9.314397753503524,\n",
       " 'l2_reg': 2.2584971571392447e-05}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce85f0",
   "metadata": {},
   "source": [
    "# XGboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d7ca76e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:11:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"l1_reg\", \"l2_reg\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\teval-rmse:343.08524\n",
      "[1]\teval-rmse:309.20435\n",
      "[2]\teval-rmse:279.41980\n",
      "[3]\teval-rmse:253.31375\n",
      "[4]\teval-rmse:230.46806\n",
      "[5]\teval-rmse:210.29347\n",
      "[6]\teval-rmse:192.69878\n",
      "[7]\teval-rmse:177.38524\n",
      "[8]\teval-rmse:164.25886\n",
      "[9]\teval-rmse:152.74907\n",
      "[10]\teval-rmse:143.07382\n",
      "[11]\teval-rmse:134.54629\n",
      "[12]\teval-rmse:127.17339\n",
      "[13]\teval-rmse:120.94813\n",
      "[14]\teval-rmse:115.79404\n",
      "[15]\teval-rmse:111.21919\n",
      "[16]\teval-rmse:107.46536\n",
      "[17]\teval-rmse:104.33890\n",
      "[18]\teval-rmse:101.63162\n",
      "[19]\teval-rmse:99.57003\n",
      "[20]\teval-rmse:97.45377\n",
      "[21]\teval-rmse:95.54520\n",
      "[22]\teval-rmse:94.37003\n",
      "[23]\teval-rmse:92.94584\n",
      "[24]\teval-rmse:91.69449\n",
      "[25]\teval-rmse:91.03733\n",
      "[26]\teval-rmse:90.24815\n",
      "[27]\teval-rmse:89.60774\n",
      "[28]\teval-rmse:89.05833\n",
      "[29]\teval-rmse:88.34713\n",
      "[30]\teval-rmse:88.08646\n",
      "[31]\teval-rmse:87.59848\n",
      "[32]\teval-rmse:87.18389\n",
      "[33]\teval-rmse:86.94881\n",
      "[34]\teval-rmse:86.59473\n",
      "[35]\teval-rmse:86.41755\n",
      "[36]\teval-rmse:86.19765\n",
      "[37]\teval-rmse:85.93733\n",
      "[38]\teval-rmse:85.82585\n",
      "[39]\teval-rmse:85.62992\n",
      "[40]\teval-rmse:85.48353\n",
      "[41]\teval-rmse:85.43597\n",
      "[42]\teval-rmse:85.34972\n",
      "[43]\teval-rmse:85.22047\n",
      "[44]\teval-rmse:85.10538\n",
      "[45]\teval-rmse:84.98559\n",
      "[46]\teval-rmse:84.96393\n",
      "[47]\teval-rmse:85.00471\n",
      "[48]\teval-rmse:84.83559\n",
      "[49]\teval-rmse:84.84797\n",
      "[50]\teval-rmse:84.76870\n",
      "[51]\teval-rmse:84.72469\n",
      "[52]\teval-rmse:84.68863\n",
      "[53]\teval-rmse:84.59606\n",
      "[54]\teval-rmse:84.59569\n",
      "[55]\teval-rmse:84.52685\n",
      "[56]\teval-rmse:84.55569\n",
      "[57]\teval-rmse:84.47573\n",
      "[58]\teval-rmse:84.49079\n",
      "[59]\teval-rmse:84.42392\n",
      "[60]\teval-rmse:84.44204\n",
      "[61]\teval-rmse:84.34316\n",
      "[62]\teval-rmse:84.32481\n",
      "[63]\teval-rmse:84.27885\n",
      "[64]\teval-rmse:84.29715\n",
      "[65]\teval-rmse:84.31818\n",
      "[66]\teval-rmse:84.23536\n",
      "[67]\teval-rmse:84.25140\n",
      "[68]\teval-rmse:84.21046\n",
      "[69]\teval-rmse:84.18822\n",
      "[70]\teval-rmse:84.18771\n",
      "[71]\teval-rmse:84.00375\n",
      "[72]\teval-rmse:83.97516\n",
      "[73]\teval-rmse:83.95360\n",
      "[74]\teval-rmse:83.88914\n",
      "[75]\teval-rmse:83.88585\n",
      "[76]\teval-rmse:83.86602\n",
      "[77]\teval-rmse:83.79900\n",
      "[78]\teval-rmse:83.76113\n",
      "[79]\teval-rmse:83.74315\n",
      "[80]\teval-rmse:83.75942\n",
      "[81]\teval-rmse:83.76002\n",
      "[82]\teval-rmse:83.75269\n",
      "[83]\teval-rmse:83.76027\n",
      "[84]\teval-rmse:83.77931\n",
      "[85]\teval-rmse:83.77183\n",
      "[86]\teval-rmse:83.60357\n",
      "[87]\teval-rmse:83.44315\n",
      "[88]\teval-rmse:83.42175\n",
      "[89]\teval-rmse:83.41235\n",
      "[90]\teval-rmse:83.37492\n",
      "[91]\teval-rmse:83.34451\n",
      "[92]\teval-rmse:83.27209\n",
      "[93]\teval-rmse:83.27177\n",
      "[94]\teval-rmse:83.24704\n",
      "[95]\teval-rmse:83.23579\n",
      "[96]\teval-rmse:83.22736\n",
      "[97]\teval-rmse:83.21648\n",
      "[98]\teval-rmse:83.22771\n",
      "[99]\teval-rmse:83.21302\n",
      "[100]\teval-rmse:83.19564\n",
      "[101]\teval-rmse:83.16673\n",
      "[102]\teval-rmse:83.07220\n",
      "[103]\teval-rmse:83.05337\n",
      "[104]\teval-rmse:82.99805\n",
      "[105]\teval-rmse:82.99490\n",
      "[106]\teval-rmse:82.93494\n",
      "[107]\teval-rmse:82.96747\n",
      "[108]\teval-rmse:82.95865\n",
      "[109]\teval-rmse:82.95748\n",
      "[110]\teval-rmse:82.92996\n",
      "[111]\teval-rmse:82.87100\n",
      "[112]\teval-rmse:82.84935\n",
      "[113]\teval-rmse:82.83200\n",
      "[114]\teval-rmse:82.83625\n",
      "[115]\teval-rmse:82.68362\n",
      "[116]\teval-rmse:82.67427\n",
      "[117]\teval-rmse:82.68301\n",
      "[118]\teval-rmse:82.68469\n",
      "[119]\teval-rmse:82.65669\n",
      "[120]\teval-rmse:82.67453\n",
      "[121]\teval-rmse:82.68002\n",
      "[122]\teval-rmse:82.70361\n",
      "[123]\teval-rmse:82.72421\n",
      "[124]\teval-rmse:82.71422\n",
      "[125]\teval-rmse:82.64621\n",
      "[126]\teval-rmse:82.60268\n",
      "[127]\teval-rmse:82.59040\n",
      "[128]\teval-rmse:82.60849\n",
      "[129]\teval-rmse:82.53397\n",
      "[130]\teval-rmse:82.54273\n",
      "[131]\teval-rmse:82.56614\n",
      "[132]\teval-rmse:82.56814\n",
      "[133]\teval-rmse:82.55567\n",
      "[134]\teval-rmse:82.55574\n",
      "[135]\teval-rmse:82.55618\n",
      "[136]\teval-rmse:82.50678\n",
      "[137]\teval-rmse:82.50979\n",
      "[138]\teval-rmse:82.56849\n",
      "[139]\teval-rmse:82.56194\n",
      "[140]\teval-rmse:82.57106\n",
      "[141]\teval-rmse:82.56594\n",
      "[142]\teval-rmse:82.56650\n",
      "[143]\teval-rmse:82.55377\n",
      "[144]\teval-rmse:82.54539\n",
      "[145]\teval-rmse:82.54934\n"
     ]
    }
   ],
   "source": [
    "mod = xgb.train(best_params, dtrain, num_boost_round=10000, evals=[(dval, 'eval')], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef33767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y_test, dtype=float)\n",
    "y_pred = np.array(mod.predict(dtest), dtype=float)\n",
    "r2_XGB = r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd0cf8",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1702ae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score for Linear Regression: 0.6314\n",
      "R^2 Score for Decision Tree: 0.8909\n",
      "R^2 Score for SVM: 0.8909\n",
      "R^2 Score for ANN: 0.8528\n",
      "R^2 Score for XGboost & Optuna: 0.9407\n"
     ]
    }
   ],
   "source": [
    "print(\"R^2 Score for Linear Regression: {:.4f}\".format(r2_linear))\n",
    "print(\"R^2 Score for Decision Tree: {:.4f}\".format(r2_tree))\n",
    "print(\"R^2 Score for SVM: {:.4f}\".format(r2_svr))\n",
    "print(\"R^2 Score for ANN: {:.4f}\".format(r2_ann))\n",
    "print(\"R^2 Score for XGboost & Optuna: {:.4f}\".format(r2_XGB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
